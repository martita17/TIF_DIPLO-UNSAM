---
title: "R Notebook"
output: html_notebook
---
#### Carga de librerías
```{r echo=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
library(tidymodels)
library(textrecipes)
library(glmnet)
library(webshot2)
library(gt)

```

#### Carga de bases
```{r}
base_topicos <- read.csv(file = "../bases/base_ejercicio3.csv")
base_notas <- read.csv(file = "../bases/corpus_clean.csv")
```

Detectamos que ambas bases tienen una longitud diferente. La base que contiene el principal tópico de cada nota tiene una observación más. Esto se debe a que la entrada con id _40158_ está duplicada debido a que a tomado el mismo porcentaje para dos tópicos diferentes.
Realizo una revisión manual para remover el tópico que cobra menos sentido al momento de revisar el contenido del artículo. Habría que corregir la forma de generar la base de tópicos para forzar a que cuando hay empate, solamente traiga uno.
Tras la limpieza, joineamos ambas bases y generamos sets según tópico para realizar pruebas.

```{r}
base_topicos <- base_topicos[-5292,]

base_global <- base_notas %>% 
  left_join(base_topicos, by = "id")

base_global <- base_global %>% 
  mutate(nom_topico = case_when(topic == 1 ~ "rel_ident",
                                topic == 2 ~ "soc_ed_salud",
                                topic == 3 ~ "pol_internac",
                                topic == 4 ~ "elecciones_nac",
                                topic == 5 ~ "tecno_redes",
                                topic == 6 ~ "chimentos_farandula",
                                topic == 7 ~ "arte_cult",
                                topic == 8 ~ "fulbo",
                                topic == 9 ~ "siniestros",
                                topic == 10 ~ "agricultura_ganaderia",
                                topic == 11 ~ "eco_pol_eco",
                                topic == 12 ~ "seguridad_judiciales")
         )

base_porcentajes <- base_global %>% 
  count(nom_topico) %>% 
  mutate(porc_topic = round(n * 100 / sum(n), digits = 2))


```
Hemos elegido los tópicos 4 (elecciones) y 12 (judiciales), los cuales representan la mayor proporción de notas del set global: el primero con un 14,99% y el segundo con un 11,50%.

Generamos las bases específicas para estos medios.

```{r}
topico4_elecciones <- base_global %>% 
  filter(nom_topico == "elecciones_nac") %>% 
  select(id, medio, orientacion, texto)

topico12_judiciales <- base_global %>% 
  filter(nom_topico == "seguridad_judiciales") %>% 
  select(id, medio, orientacion, texto)

topico4_elecciones %>% 
  count(orientacion)

topico12_judiciales %>% 
  count(orientacion)
```


Dado que estamos tratando con una variable multicategorial, exploramos la posibilidad de realizar modelos predictores por categoría y luego hacerlos competir. Para ello, generamos 3 nuevas variables dicotómicas a partir de la variable _orientacion_, las cuales permiten saber si cada nota pertenece o no a una determinada categoría.

```{r}
topico4_elecciones <- topico4_elecciones %>% 
  mutate(conservador = case_when(orientacion == "+ conservador" ~ "si",
                                 orientacion != "+ conservador" ~ "no"),
         neutro = case_when(orientacion == "neutro" ~ "si",
                            orientacion != "neutro" ~ "no"),
         progresista = case_when(orientacion == "+ progresista" ~ "si",
                                 orientacion != "+ progresista" ~ "no"))
```




Para que la base funcione con los embeddings en español, es necesaria una limpieza más superficial: no se le quitan mayúsculas ni caractéres no ascii
```{r}
topico4_elecciones_clean <- topico4_elecciones %>%
        mutate(texto = str_replace_all(texto, "'\\[.*?¿\\]\\%'", " ")) %>%
        mutate(texto = str_replace_all(texto, "[[:digit:]]+", "DIGITO")) #%>% 
        mutate(medio = as.numeric(factor(medio))) # POR QUÉ PASAMOS A FACTOR ESTA VARIABLE¿?

topico12_judiciales_clean <- topico12_judiciales %>%
        mutate(texto = str_replace_all(texto, "'\\[.*?¿\\]\\%'", " ")) %>%
        mutate(texto = str_replace_all(texto, "[[:digit:]]+", "DIGITO")) %>% 
        mutate(medio = as.numeric(factor(medio)))
```

Cargo los embeddings sugeridos
```{r embeddings}
load_embeddings <- function(path=NULL, type=c("w2v", "ft")){
        if (type=="w2v"){
                embedding <- word2vec::read.wordvectors(path, 
                                                        type = "bin", 
                                                        normalize = TRUE) %>%
                        as_tibble(rownames="word")
        }
        else if (type=="ft"){
                model <- fastTextR::ft_load(path)
                words <- fastTextR::ft_words(model)
                embedding <- fastTextR::ft_word_vectors(model,
                                                        words) %>%
                        as_tibble(rownames="word")
        }
        
        return(embedding)
}

embedding <- load_embeddings(
  path = "../bases/sbw_vectors.bin",
  type = "w2v"
)


```

tidy models me permite usar recipies también para construir la matriz de embeddings que será insumo para el modelo. Por lo que le indicamos...se tokenizará todo menos la columna document (que le estamos dando el rol de ID) y la columna orientacion.
```{r recipe}
notas_rec_embed <- recipe(orientacion ~ ., data = topico4_elecciones_clean) %>%
        update_role("id", new_role = "ID") %>% 
        step_tokenize(texto) %>% 
        step_word_embeddings(texto, 
                             embeddings=embedding,
                             aggregation = "mean")
```

Se aplica la receta
```{r prep}
tictoc::tic()
not_embed <- notas_rec_embed %>% prep() %>% bake(topico4_elecciones_clean)
tictoc::toc()
```

Divido el dataset en validation, train y test para el modelo de progresion logaritmica
```{r}
set.seed(664)
notas_split <- initial_split(not_embed, strata = orientacion)
train_embed <- training(notas_split)
test_embed <- testing(notas_split)

```

 A continuación eligo el modelo, en esta caso progresión logarítmica tipo lasso. 
```{r modelo}
# lasso_spec_multi <- multinom_reg(
#         penalty = tune(),
#         mixture = 1) %>%
#         set_mode("classification") %>%
#         set_engine("glmnet")

lasso_spec_log <- logistic_reg(
        penalty = tune(),
        mixture = 1) %>%
        set_mode("classification") %>%
        set_engine("glmnet")
```
 
A continuación seteos varios: construyo un recipe y un workflow y una grilla de hiperparámetros, esta le indicará distintas combinaciones de parámetros, así nos quedamos con la que mejor funciona. También seteo la validación cruzada
```{r}
notas_rec_embed <-
        recipe(conservador ~ ., data = train_embed) %>%
        update_role(all_of(c("id", "medio", "neutro", "progresista", "orientacion")), new_role = "ID")


wf_embed <- workflow() %>% 
        add_recipe(notas_rec_embed) %>%
        add_model(lasso_spec_log)


grid_lasso <- grid_regular(penalty(), levels = 10)


set.seed(234)
embed_folds <- vfold_cv(train_embed, v = 5)

```
 
 Entrenamiento
```{r}
tictoc::tic()
tune_lasso_embed <- tune_grid(
        wf_embed,
        embed_folds,
        grid = grid_lasso,
        control = control_resamples(save_pred = TRUE)
)
tictoc::toc()
```
 
```{r}
collect_metrics(tune_lasso_embed)
```
 
 
 
```{r}
show_best(tune_lasso_embed) # muestra los 4 mejores por defecto
```

```{r}
chosen_auc_embed <- tune_lasso_embed %>%
  select_by_one_std_err(metric = "roc_auc", -penalty)

chosen_auc_embed
```

```{r}
final_params_lasso_embed <- finalize_workflow(wf_embed, chosen_auc_embed)
final_params_lasso_embed

```
Corremos el mejor modelo
```{r}
fitted_lasso_embed <- fit(final_params_lasso_embed, train_embed)
```

Hacemos el testeo con mejor modelo que teníamos
```{r}
preds_embed <- test_embed %>%
        select(id, orientacion, conservador) %>%
        bind_cols(predict(fitted_lasso_embed, test_embed, type="prob")) %>%
        bind_cols(predict(fitted_lasso_embed, test_embed, type="class"))
```

```{r}
glimpse(preds_embed)
```


Sacamos métricas del modelo       
```{r}
preds_embed <- preds_embed %>% 
  rename(.pred_conservador = `.pred_+ conservador`,
         .pred_progresista = `.pred_+ progresista`)

metricas_conservador <- roc_auc(preds_embed, conservador, .pred_no) %>%
bind_rows(accuracy(preds_embed, conservador, .pred_class)) %>%
bind_rows(precision(preds_embed, conservador, .pred_class)) %>%
bind_rows(recall(preds_embed, conservador, .pred_class)) %>%
bind_rows(f_meas(preds_embed, conservador, .pred_class))

tabla_conservador <- metricas_conservador %>% 
  select(-.estimator) %>% 
  gt() %>% 
  tab_header(
    title = md("Métricas de performance - conservador"),  
    subtitle = md("Regresión logística con tópico 4"))%>%
  cols_label(
    .metric = md("**Metrica**"),   # Rename .metric to "Metric" (bold)
    .estimate = md("**Estimado**") # Rename .estimate to "Estimate" (bold)
  ) %>%
  fmt_number(columns = c(.estimate), decimals = 3) 

gtsave(tabla_conservador, "../visualizaciones/regresion-log/tabla_topico4_conservador.png")
```

Pareciera que lo que el modelo mejor hace es predecir positivos; es decir, acertar a clasificar un caso como conservador cuando este efectivamente lo es.


#### Probamos con las otras dos categorías
#### Neutro

A continuación seteos varios: construyo un recipe y un workflow y una grilla de hiperparámetros, esta le indicará distintas combinaciones de parámetros, así nos quedamos con la que mejor funciona. También seteo la validación cruzada
```{r}
notas_rec_embed_neutro <-
        recipe(neutro ~ ., data = train_embed) %>%
        update_role(all_of(c("id", "medio", "conservador", "progresista", "orientacion")), new_role = "ID")


wf_embed_neutro <- workflow() %>% 
        add_recipe(notas_rec_embed_neutro) %>%
        add_model(lasso_spec_log) #lasso _spec_log no se modifica

# No se modifica respecto al anterior
# grid_lasso <- grid_regular(penalty(), levels = 10)
# 
# set.seed(234)
embed_folds_neutro <- vfold_cv(train_embed, v = 10)

```
 
 Entrenamiento
```{r}
tictoc::tic()
tune_lasso_embed_neutro <- tune_grid(
        wf_embed_neutro, #Solo se modifica este término
        embed_folds_neutro,
        grid = grid_lasso,
        control = control_resamples(save_pred = TRUE)
)
tictoc::toc()
```
 
```{r}
collect_metrics(tune_lasso_embed_neutro)
```
 
 
 
```{r}
show_best(tune_lasso_embed_neutro) # muestra los 4 mejores por defecto
```
SI CORREMOS TODO IGUAL, SOLAMENTE MODIFICANDO LOS TÉRMINOS ASOCIADOS A LA RECIPE (DADO QUE CAMBIAMOS LA VARIABLE A PREDECIR Y LAS VARIABLES CATEGORIZADAS COMO ID (PARA QUE SEAN IGNORADAS COMO VARIABLE PREDICTORAS) COMPROBAMOS QUE EL PENATY NO SE MODIFICA. POSIBLEMENTE NO SEA NECESARIO VOLVER A CALCULARLO.
PROBAMOS AUMENTANDO LA CANTIDAD DE CRUCES Y EL PENALTY RESULTANTE ES EL MISMO.


EN PRINCIPIO EL SIGUIENTE PASO NO SERÍA NECESARIO, DADO QUE EL PENALTY ES EL MISMO

```{r}
chosen_auc_embed <- tune_lasso_embed %>%
  select_by_one_std_err(metric = "roc_auc", -penalty)

chosen_auc_embed
```

```{r}
final_params_lasso_embed_neutro <- finalize_workflow(wf_embed_neutro, chosen_auc_embed)
final_params_lasso_embed

```
Corremos el mejor modelo
```{r}
fitted_lasso_embed_neutro <- fit(final_params_lasso_embed_neutro, train_embed)
```

Hacemos el testeo con mejor modelo que teníamos
```{r}
preds_embed_neutro <- test_embed %>%
        select(id, orientacion, neutro) %>%
        bind_cols(predict(fitted_lasso_embed_neutro, test_embed, type="prob")) %>%
        bind_cols(predict(fitted_lasso_embed_neutro, test_embed, type="class"))
```

```{r}
glimpse(preds_embed_neutro)
```


Sacamos métricas del modelo...NO ME FUNCIONA ESTO       
```{r}
# preds_embed <- preds_embed %>% 
#   rename(.pred_conservador = `.pred_+ conservador`,
#          .pred_progresista = `.pred_+ progresista`)

metricas_neutro <- roc_auc(preds_embed_neutro, neutro, .pred_no) %>%
bind_rows(accuracy(preds_embed_neutro, neutro, .pred_class)) %>%
bind_rows(precision(preds_embed_neutro, neutro, .pred_class)) %>%
bind_rows(recall(preds_embed_neutro, neutro, .pred_class)) %>%
bind_rows(f_meas(preds_embed_neutro, neutro, .pred_class))

tabla_neutro <- metricas_neutro %>% 
  select(-.estimator) %>% 
  gt() %>% 
  tab_header(
    title = md("Métricas de performance - neutro"),  
    subtitle = md("Regresión logística con tópico 4"))%>%
  cols_label(
    .metric = md("**Metrica**"),   # Rename .metric to "Metric" (bold)
    .estimate = md("**Estimado**") # Rename .estimate to "Estimate" (bold)
  ) %>%
  fmt_number(columns = c(.estimate), decimals = 3) 

gtsave(tabla_neutro, "../visualizaciones/regresion-log/tabla_topico4_neutro.png")

```


#### Progresista

A continuación seteos varios: construyo un recipe y un workflow y una grilla de hiperparámetros, esta le indicará distintas combinaciones de parámetros, así nos quedamos con la que mejor funciona. También seteo la validación cruzada
```{r}
notas_rec_embed_progresista <-
        recipe(progresista ~ ., data = train_embed) %>%
        update_role(all_of(c("id", "medio", "conservador", "neutro", "orientacion")), new_role = "ID")


wf_embed_progresista <- workflow() %>% 
        add_recipe(notas_rec_embed_progresista) %>%
        add_model(lasso_spec_log) #lasso _spec_log no se modifica

# No se modifica respecto al anterior
# grid_lasso <- grid_regular(penalty(), levels = 10)
# 
# set.seed(234)
embed_folds_progresista <- vfold_cv(train_embed, v = 10)

```
 
 Entrenamiento
```{r}
tictoc::tic()
tune_lasso_embed_progresista <- tune_grid(
        wf_embed_progresista, #Solo se modifica este término
        embed_folds_progresista,
        grid = grid_lasso,
        control = control_resamples(save_pred = TRUE)
)
tictoc::toc()
```
 
```{r}
collect_metrics(tune_lasso_embed_progresista)
```
 
 
 
```{r}
show_best(tune_lasso_embed_progresista) # muestra los 4 mejores por defecto
```
SI CORREMOS TODO IGUAL, SOLAMENTE MODIFICANDO LOS TÉRMINOS ASOCIADOS A LA RECIPE (DADO QUE CAMBIAMOS LA VARIABLE A PREDECIR Y LAS VARIABLES CATEGORIZADAS COMO ID (PARA QUE SEAN IGNORADAS COMO VARIABLE PREDICTORAS) COMPROBAMOS QUE EL PENATY NO SE MODIFICA. POSIBLEMENTE NO SEA NECESARIO VOLVER A CALCULARLO.
PROBAMOS AUMENTANDO LA CANTIDAD DE CRUCES Y EL PENALTY RESULTANTE ES EL MISMO.


EN PRINCIPIO EL SIGUIENTE PASO NO SERÍA NECESARIO, DADO QUE EL PENALTY ES EL MISMO

```{r}
chosen_auc_embed <- tune_lasso_embed %>%
  select_by_one_std_err(metric = "roc_auc", -penalty)

chosen_auc_embed
```

```{r}
final_params_lasso_embed_progresista <- finalize_workflow(wf_embed_progresista, chosen_auc_embed)
final_params_lasso_embed

```
Corremos el mejor modelo
```{r}
fitted_lasso_embed_progresista <- fit(final_params_lasso_embed_progresista, train_embed)
```

Hacemos el testeo con mejor modelo que teníamos
```{r}
preds_embed_progresista <- test_embed %>%
        select(id, orientacion, progresista) %>%
        bind_cols(predict(fitted_lasso_embed_progresista, test_embed, type="prob")) %>%
        bind_cols(predict(fitted_lasso_embed_progresista, test_embed, type="class"))
```

```{r}
glimpse(preds_embed_progresista)
```


Sacamos métricas del modelo. 
```{r}
# preds_embed <- preds_embed %>% 
#   rename(.pred_conservador = `.pred_+ conservador`,
#          .pred_progresista = `.pred_+ progresista`)

metricas_prog <- roc_auc(preds_embed_progresista, progresista, .pred_no) %>%
bind_rows(accuracy(preds_embed_progresista, progresista, .pred_class)) %>%
bind_rows(precision(preds_embed_progresista, progresista, .pred_class)) %>%
bind_rows(recall(preds_embed_progresista, progresista, .pred_class)) %>%
bind_rows(f_meas(preds_embed_progresista, progresista, .pred_class))

tabla_progre <- metricas_prog %>% 
  select(-.estimator) %>% 
  gt() %>% 
  tab_header(
    title = md("Métricas de performance - progresista"),  
    subtitle = md("Regresión logística con tópico 4"))%>%
  cols_label(
    .metric = md("**Metrica**"),   # Rename .metric to "Metric" (bold)
    .estimate = md("**Estimado**") # Rename .estimate to "Estimate" (bold)
  ) %>%
  fmt_number(columns = c(.estimate), decimals = 3) 

gtsave(tabla_progre, "../visualizaciones/regresion-log/tabla_topico4_progre.png")

```



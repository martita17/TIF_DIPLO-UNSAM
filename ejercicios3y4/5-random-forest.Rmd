---
title: "Random Forest"
output: html_document
date: "`r Sys.Date()`"
---

#### Carga de librerías
```{r}
library(tidyverse)
library(tidytext)
library(tidymodels)
library(textrecipes)
library(glmnet)
library(webshot2)
library(gt)
library(themis)
#library(doParallel)
library(ranger)
```


#### Carga de bases
```{r}
base_topicos <- read.csv(file = "../bases/base_ejercicio3.csv")
base_notas <- read.csv(file = "../bases/corpus_clean.csv")
```

Detectamos que ambas bases tienen una longitud diferente. La base que contiene el principal tópico de cada nota tiene una observación más. Esto se debe a que la entrada con id _40158_ está duplicada debido a que a tomado el mismo porcentaje para dos tópicos diferentes.
Realizo una revisión manual para remover el tópico que cobra menos sentido al momento de revisar el contenido del artículo. Habría que corregir la forma de generar la base de tópicos para forzar a que cuando hay empate, solamente traiga uno.
Tras la limpieza, joineamos ambas bases y generamos sets según tópico para realizar pruebas.

```{r}
base_topicos <- base_topicos[-5292,]

base_global <- base_notas %>% 
  left_join(base_topicos, by = "id")

base_global <- base_global %>% 
  mutate(nom_topico = case_when(topic == 1 ~ "rel_ident",
                                topic == 2 ~ "soc_ed_salud",
                                topic == 3 ~ "pol_internac",
                                topic == 4 ~ "elecciones_nac",
                                topic == 5 ~ "tecno_redes",
                                topic == 6 ~ "chimentos_farandula",
                                topic == 7 ~ "arte_cult",
                                topic == 8 ~ "fulbo",
                                topic == 9 ~ "siniestros",
                                topic == 10 ~ "agricultura_ganaderia",
                                topic == 11 ~ "eco_pol_eco",
                                topic == 12 ~ "seguridad_judiciales")
         )

base_porcentajes <- base_global %>% 
  count(nom_topico) %>% 
  mutate(porc_topic = round(n * 100 / sum(n), digits = 2))


```
Hemos elegido los tópicos 4 (elecciones) y 12 (judiciales), los cuales representan la mayor proporción de notas del set global: el primero con un 14,99% y el segundo con un 11,50%.

Generamos las bases específicas para estos medios.

#### TÓPICO 4 - ELECCIONES NACIONALES
```{r}
topico4_elecciones <- base_global %>% 
  filter(nom_topico == "elecciones_nac") %>% 
  select(id, orientacion, texto) 

topico4_elecciones %>% 
  count(orientacion)
```

Modificamos las categorías (para mayor comodidad) y limpiamos el texto para pasarle los embeddings
```{r}
topico4_elecciones <- topico4_elecciones %>% 
  mutate(orientacion = case_when(orientacion == "+ conservador" ~ "conservador",
                                 orientacion == "+ progresista" ~ "progresista",
                                 TRUE ~ orientacion))

topico4_elecciones_clean <- topico4_elecciones %>%
        mutate(texto = str_replace_all(texto, "'\\[.*?¿\\]\\%'", " ")) %>%
        mutate(texto = str_replace_all(texto, "[[:digit:]]+", "DIGITO"))
```


#### Cargamos los embeddings
```{r embeddings}
load_embeddings <- function(path=NULL, type=c("w2v", "ft")){
        if (type=="w2v"){
                embedding <- word2vec::read.wordvectors(path, 
                                                        type = "bin", 
                                                        normalize = TRUE) %>%
                        as_tibble(rownames="word")
        }
        else if (type=="ft"){
                model <- fastTextR::ft_load(path)
                words <- fastTextR::ft_words(model)
                embedding <- fastTextR::ft_word_vectors(model,
                                                        words) %>%
                        as_tibble(rownames="word")
        }
        
        return(embedding)
}

embedding <- load_embeddings(
  path = "../bases/sbw_vectors.bin",
  type = "w2v"
)
```
#### Generamos la receta y aplicamos sobre el tópico 4
Esto lo hacemos antes de generar la receta del random forest, debido a que necesitamos que tanto la base de entrenamiento, como la base de testeo sean homogéneas en relación a los embeddings.

```{r}
receta_embedd <- recipe(orientacion ~ ., data = topico4_elecciones_clean) %>%
  update_role("id", new_role = "ID") %>% 
  step_tokenize(texto) %>% 
  step_word_embeddings(texto, 
                       embeddings=embedding,
                       aggregation = "mean")

tictoc::tic()
topico4_embedd <- receta_embedd %>% prep() %>% bake(topico4_elecciones_clean)
tictoc::toc()
```
#### Realizamos el split de la base
```{r}
set.seed(664)
notas_split_t4 <- initial_split(topico4_embedd, strata = orientacion)
bd_train_t4 <- training(notas_split_t4)
bd_test_t4 <- testing(notas_split_t4)
```


#### Comenzamos a diseñar la receta para el Random Forest (por ahora sin el modelo)
```{r}
receta_rf <- recipe(orientacion ~ ., data = bd_train_t4)%>%
  update_role(id, new_role = "id") %>%
  step_downsample(orientacion, under_ratio = 1)
```


#### Establecemos el modelo
Fijamos en 200 la cantidad de árboles y dejamos por determinar los hiperparámetros mtry (cantidad de variables predictoras que se seleccionan en cada división) y min_n (cantidad de observaciones mínimas en un nodo para realizar una nueva división).
Generamos el work flow con nuestra receta y nuestro modelo.
```{r}
modelo_rf <- rand_forest(
  trees = 200,
  mtry = tune(),
  min_n = tune()
  ) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

work_flow_rf <- workflow() %>% 
  add_recipe(receta_rf) %>% 
  add_model(modelo_rf)
```


```{r}
#doParallel::registerDoParallel() #permite paralelizar el proceso de ajuste de hiperparámetros, reduciendo el tiempo de entrenamiento

# Definimos una cuadrícula de búsqueda, ganando control sobre los posible valores que asume cada hiperparámetro
cuadricula <- grid_regular(
  mtry(range = c(10, 100)),  # Rango para mtry
  min_n(range = c(2, 20)),   # Rango para min_n
  levels = 5  # Número de valores a probar para cada hiperparámetro
)

# Ajustar el modelo con validación cruzada
set.seed(664)  # Para reproducibilidad
resultados_ajuste <- tune_grid(
  work_flow_rf,
  resamples = vfold_cv(bd_train_t4, v = 5),  # Validación cruzada con 5 folds (nuestra base es pequeña)
  grid = cuadricula,
  metrics = metric_set(roc_auc, accuracy, precision, recall)  # Métricas a optimizar
)

# Seleccionamos los mejores hiperparámetros
mejores_hiperparametros <- select_best(resultados_ajuste, metric = "roc_auc")

mejores_hiperparametros

```
Nos ha seleccionado los hiperparámetros más altos aportados por la grilla. Esto parece indicarnos que podemos hacer crecer esos hiperparámetro bastante. Haemos otra prueba.


```{r}
set.seed(664)  # Semilla para reproducibilidad


resultados_ajuste2 <- tune_grid(work_flow_rf,
                         resamples = vfold_cv(bd_train_t4),  # Validación cruzada. 10 folds por defecto.
                         grid = 10,  # Número de combinaciones aleatorias
                         metrics = metric_set(roc_auc, accuracy, precision, recall))

mejores_hiperparametros2 <- select_best(resultados_ajuste2, metric = "roc_auc")

mejores_hiperparametros2
```
Comparamos las métricas obtenidas
```{r}
show_best(resultados_ajuste)
```

```{r}
show_best(resultados_ajuste2)
```
Vemos que las métricas arrojadas por la segunda prueba mejoran tanto a nivel de la media como del standard error.


```{r}
# Finalizar el modelo con los mejores hiperparámetros
modelo_final <- finalize_workflow(work_flow_rf, mejores_hiperparametros2) %>%
  fit(bd_train_t4)  # Entrenar el modelo final

modelo_final
```

#### Testeamos
```{r}
test_val <- modelo_final %>%
  predict(bd_test_t4) %>%
  bind_cols(., bd_test_t4)

test_val <- test_val %>% 
  select(1:3) %>% 
  rename(prediccion = ".pred_class")

test_val <- predict(modelo_final, bd_test_t4, type = "prob") %>%
  bind_cols(test_val, .)

metrics_random <- roc_auc(test_val, truth = orientacion, .pred_conservador, .pred_neutro, .pred_progresista) %>%
  bind_rows(accuracy(test_val, truth = orientacion, estimate = prediccion)) %>%
  bind_rows(precision(test_val, truth = orientacion, estimate = prediccion)) %>%
  bind_rows(recall(test_val, truth = orientacion, estimate = prediccion)) %>%
  bind_rows(f_meas(test_val, truth = orientacion, estimate = prediccion)) %>%
  mutate(modelo = "random_forest")

tabla_rf <- metrics_random %>% 
  select(-.estimator) %>% 
  gt() %>% 
  tab_header(
    title = md("Métricas de performance - Multiclase"),  
    subtitle = md("Random forest con tópico 4"))%>%
  cols_label(
    .metric = md("**Metrica**"),   # Rename .metric to "Metric" (bold)
    .estimate = md("**Estimado**") # Rename .estimate to "Estimate" (bold)
  ) %>%
  fmt_number(columns = c(.estimate), decimals = 3) 

gtsave(tabla_rf, "../visualizaciones/regresion-log/tabla_t4_rf.png")

# metrics_random <- roc_auc(test_val, truth = orientacion, .pred_conservador, .pred_neutro, .pred_progresista) %>%  # ROC AUC para "conservador"
#   bind_rows(class_metrics(test_val, truth = orientacion, estimate = prediccion)) %>%  # Otras métricas
#   mutate(modelo = "random_forest")  # Agregar nombre del modelo


```



#### TÓPICO 12 - SEGURIDAD Y JUDICIALES
```{r}
topico12_judiciales <- base_global %>% 
  filter(nom_topico == "seguridad_judiciales") %>% 
  select(id, medio, orientacion, texto)

topico12_judiciales %>% 
  count(orientacion)
```









